{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEkrSo01WcC7"
      },
      "source": [
        "# TensorFlow & Keras Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned TensorFlow fundamentals, GradientTape basics, and the high-level Keras API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only tf.Variable |\n",
        "| **IV** | Custom Keras Layers | Proper subclassing with `build()` and `call()` |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training with GradientTape |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQaXbL0qWcC7",
        "outputId": "13f701cc-0215-4bce-873f-3c294671e5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Keras Version:      3.10.0\n",
            "GPU Available:      False\n",
            "\n",
            "Ready for Advanced TensorFlow & Keras!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, Model, Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version:      {keras.__version__}\")\n",
        "print(f\"GPU Available:      {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced TensorFlow & Keras!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tItbO-4WcC8"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced GradientTape Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used GradientTape for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested tapes** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** for non-differentiable operations\n",
        "- **Gradient clipping** and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9-VhzBFWcC8",
        "outputId": "c325fcbd-dfc2-4f64-e705-9e57c9b42d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED TAPES: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED GRADIENTTAPES: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED TAPES: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape3:\n",
        "    with tf.GradientTape() as tape2:\n",
        "        with tf.GradientTape() as tape1:\n",
        "            y = x ** 4\n",
        "        dy_dx = tape1.gradient(y, x)      # First derivative: 4x^3\n",
        "    d2y_dx2 = tape2.gradient(dy_dx, x)    # Second derivative: 12x^2\n",
        "d3y_dx3 = tape3.gradient(d2y_dx2, x)      # Third derivative: 24x\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.numpy()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.numpy():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.numpy():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.numpy():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.numpy():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSJanpUWcC8",
        "outputId": "b4398de8-6e8b-4e87-87d0-9c29f8ca7e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1. 2. 3.]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.      2.      0.14112]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = tf.Variable([1.0, 2.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "    y = tf.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        tf.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "print(f\"\\nx = {x.numpy()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {y.numpy()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_a3dkRlWcC8",
        "outputId": "71debf36-ae24-4b66-ec53-4813adfd2557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [ 4. 13.]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = tf.Variable([1.0, 2.0])\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "        # Scalar function: f(x, y) = x^2*y + y^3\n",
        "        f = x[0]**2 * x[1] + x[1]**3\n",
        "    grad = tape1.gradient(f, x)  # [2xy, x^2 + 3y^2]\n",
        "hessian = tape2.jacobian(grad, x)\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].numpy()}, {x[1].numpy()})\")\n",
        "print(f\"f = {f.numpy()}\")\n",
        "print(f\"\\nGradient: {grad.numpy()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srxJFP_KWcC9",
        "outputId": "d5f1e24e-2907-4e01-e008-4d2ae8354e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              CUSTOM GRADIENTS\n",
            "============================================================\n",
            "\n",
            "Input: [3. 4.]\n",
            "Gradient (clipped to norm 1.0): [0.70710677 0.70710677]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM GRADIENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              CUSTOM GRADIENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sometimes you need to define custom gradients:\n",
        "# - For non-differentiable operations (like argmax)\n",
        "# - For numerical stability\n",
        "# - For custom backward passes (like straight-through estimators)\n",
        "\n",
        "@tf.custom_gradient\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Clip the incoming gradient\n",
        "        norm = tf.norm(dy)\n",
        "        clipped = tf.cond(\n",
        "            norm > clip_value,\n",
        "            lambda: dy * clip_value / norm,\n",
        "            lambda: dy\n",
        "        )\n",
        "        return clipped  # Only return gradient for 'x'\n",
        "    return x, grad\n",
        "\n",
        "# Test custom gradient\n",
        "x = tf.Variable([3.0, 4.0])  # Gradient will have norm 5 (3-4-5 triangle)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = clip_gradient_norm(x, clip_value=1.0)\n",
        "    loss = tf.reduce_sum(y)  # Gradient would be [1, 1] but we pass [3, 4]\n",
        "\n",
        "# Manually set upstream gradient to [3, 4]\n",
        "grad = tape.gradient(loss, x)\n",
        "print(f\"\\nInput: {x.numpy()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {grad.numpy()}\")\n",
        "print(f\"Gradient norm: {tf.norm(grad).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8-8EpEWcC9",
        "outputId": "a1a3422e-a130-4616-edc0-ab4b2bf203ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.3 0.7 1.2 2.5]\n",
            "Rounded: [0. 1. 1. 2.]\n",
            "Gradient (straight-through): [0. 2. 2. 4.]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_round(x):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        return dy  # Straight-through: gradient = identity\n",
        "    return tf.round(x), grad\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_sign(x):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return dy * tf.cast(tf.abs(x) <= 1, dy.dtype)\n",
        "    return tf.sign(x), grad\n",
        "\n",
        "# Test\n",
        "x = tf.Variable([0.3, 0.7, 1.2, 2.5])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = straight_through_round(x)\n",
        "    loss = tf.reduce_sum(y ** 2)\n",
        "\n",
        "grad = tape.gradient(loss, x)\n",
        "\n",
        "print(f\"\\nInput:   {x.numpy()}\")\n",
        "print(f\"Rounded: {y.numpy()}\")\n",
        "print(f\"Gradient (straight-through): {grad.numpy()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uwxn2-LWcC9",
        "outputId": "f5c7841a-f044-44dc-fcb9-5ba212ccddef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. Compute gradients for mini-batch\n",
            "  2. Accumulate (sum or average) over N steps\n",
            "  3. Apply accumulated gradients once\n",
            "  4. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, training=True)\n",
        "            loss = tf.reduce_mean(keras.losses.mse(y_batch, predictions))\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Accumulate (average over steps)\n",
        "        accumulated_gradients = [\n",
        "            acc + grad / accumulation_steps\n",
        "            for acc, grad in zip(accumulated_gradients, gradients)\n",
        "        ]\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. Compute gradients for mini-batch\")\n",
        "print(\"  2. Accumulate (sum or average) over N steps\")\n",
        "print(\"  3. Apply accumulated gradients once\")\n",
        "print(\"  4. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCMnLwcWcC9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using Keras layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-nVGvvWcC-",
        "outputId": "a2382347-f1c4-48df-d5cd-5656c12cdd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  (1, 5, 5, 1)\n",
            "Kernel shape: (3, 3, 1, 2)\n",
            "Output shape: (1, 3, 3, 2)\n",
            "Matches tf.nn.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding='VALID'):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, height, width, in_channels)\n",
        "    kernel : tensor (kernel_h, kernel_w, in_channels, out_channels)\n",
        "    stride : int\n",
        "    padding : 'VALID' or 'SAME'\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w = input_tensor.shape[1], input_tensor.shape[2]\n",
        "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
        "    out_channels = kernel.shape[3]\n",
        "\n",
        "    if padding == 'SAME':\n",
        "        pad_h = k_h // 2\n",
        "        pad_w = k_w // 2\n",
        "        input_tensor = tf.pad(input_tensor,\n",
        "                              [[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
        "        in_h += 2 * pad_h\n",
        "        in_w += 2 * pad_w\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = tf.TensorArray(dtype=tf.float32, size=out_h * out_w)\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            # Extract patch\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            patch = input_tensor[:, h_start:h_start+k_h, w_start:w_start+k_w, :]\n",
        "\n",
        "            # Convolve: sum over (h, w, in_channels), keep out_channels\n",
        "            # patch: (batch, k_h, k_w, in_c)\n",
        "            # kernel: (k_h, k_w, in_c, out_c)\n",
        "            conv = tf.einsum('bhwi,hwio->bo', patch, kernel)\n",
        "            output = output.write(idx, conv)\n",
        "            idx += 1\n",
        "\n",
        "    output = output.stack()  # (out_h*out_w, batch, out_c)\n",
        "    output = tf.transpose(output, [1, 0, 2])  # (batch, out_h*out_w, out_c)\n",
        "    output = tf.reshape(output, [batch_size, out_h, out_w, out_channels])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = tf.random.normal((1, 5, 5, 1))  # 1 image, 5x5, 1 channel\n",
        "kernel = tf.random.normal((3, 3, 1, 2))  # 3x3 kernel, 1->2 channels\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding='VALID')\n",
        "tf_output = tf.nn.conv2d(x, kernel, strides=1, padding='VALID')\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches tf.nn.conv2d: {tf.reduce_all(tf.abs(our_output - tf_output) < 1e-5).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjjl1lJWcC-",
        "outputId": "95712523-fc22-48df-90f8-c42fbb791c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (1, 4, 4, 2)\n",
            "Input (channel 0):\n",
            "[[ 1.  3.  5.  7.]\n",
            " [ 9. 11. 13. 15.]\n",
            " [17. 19. 21. 23.]\n",
            " [25. 27. 29. 31.]]\n",
            "\n",
            "Output shape: (1, 2, 2, 2)\n",
            "Output (channel 0):\n",
            "[[11. 15.]\n",
            " [27. 31.]]\n",
            "\n",
            "Matches tf.nn.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w, channels = input_tensor.shape[1:]\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(out_h):\n",
        "        row = []\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size, :]\n",
        "            # Max over spatial dimensions\n",
        "            pooled = tf.reduce_max(window, axis=[1, 2])\n",
        "            row.append(pooled)\n",
        "        outputs.append(tf.stack(row, axis=1))\n",
        "\n",
        "    return tf.stack(outputs, axis=1)\n",
        "\n",
        "# Test\n",
        "x = tf.constant([[[[1., 2.], [3., 4.], [5., 6.], [7., 8.]],\n",
        "                  [[9., 10.], [11., 12.], [13., 14.], [15., 16.]],\n",
        "                  [[17., 18.], [19., 20.], [21., 22.], [23., 24.]],\n",
        "                  [[25., 26.], [27., 28.], [29., 30.], [31., 32.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input (channel 0):\")\n",
        "print(x[0, :, :, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "tf_pool = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output (channel 0):\")\n",
        "print(our_pool[0, :, :, 0].numpy())\n",
        "print(f\"\\nMatches tf.nn.max_pool2d: {tf.reduce_all(our_pool == tf_pool).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeZuo5ITWcC-",
        "outputId": "8b0ca142-d889-4222-ff56-ed7aa952fcd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (8, 4)\n",
            "Input mean per feature: [ 0.32574826 -0.22160122  0.37827352  0.3575499 ]\n",
            "Input std per feature:  [0.8102391  0.74234474 1.061351   1.0821929 ]\n",
            "\n",
            "Output (training) mean: [-2.9802322e-08  4.6566129e-08  7.4505806e-09 -2.2351742e-08]\n",
            "Output (training) std:  [0.9999923 0.9999908 0.9999955 0.9999958]\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(num_features), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(num_features), name='beta')\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = tf.Variable(tf.zeros(num_features), trainable=False)\n",
        "        self.running_var = tf.Variable(tf.ones(num_features), trainable=False)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = tf.reduce_mean(x, axis=0)\n",
        "            batch_var = tf.math.reduce_variance(x, axis=0)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean.assign(\n",
        "                (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            )\n",
        "            self.running_var.assign(\n",
        "                (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "            )\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = tf.random.normal((8, 4))  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {tf.reduce_mean(x, axis=0).numpy()}\")\n",
        "print(f\"Input std per feature:  {tf.math.reduce_std(x, axis=0).numpy()}\")\n",
        "print(f\"\\nOutput (training) mean: {tf.reduce_mean(y_train, axis=0).numpy()}\")\n",
        "print(f\"Output (training) std:  {tf.math.reduce_std(y_train, axis=0).numpy()}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR_REX3RWcC-",
        "outputId": "7b5c6775-694a-4c16-b022-6a0bbcb5e41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (2, 3, 4)\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [ 0.65648675 -0.4130517   0.33997506 -1.0056272 ]\n",
            "  Output: [ 1.1744822  -0.47392502  0.6866642  -1.3872216 ]\n",
            "  Output mean: -0.000000\n",
            "  Output std:  1.0000\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(normalized_shape), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(normalized_shape), name='beta')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = tf.random.normal((2, 3, 4))  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].numpy()}\")\n",
        "print(f\"  Output: {y[0, 0, :].numpy()}\")\n",
        "print(f\"  Output mean: {tf.reduce_mean(y[0, 0, :]).numpy():.6f}\")\n",
        "print(f\"  Output std:  {tf.math.reduce_std(y[0, 0, :]).numpy():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Ok8o68WcC_",
        "outputId": "b0c84716-1118-471c-c6c2-28a1befba583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape (2, 10)\n",
            "\n",
            "Dropout sample 1: [0. 2. 2. 0. 0. 2. 0. 2. 2. 0.]\n",
            "Dropout sample 2: [0. 2. 2. 2. 0. 0. 2. 2. 2. 0.]\n",
            "Dropout sample 3: [2. 0. 2. 2. 2. 0. 0. 2. 2. 2.]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "Average over 1000 samples: 1.0045 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = tf.cast(\n",
        "        tf.random.uniform(tf.shape(x)) < keep_prob,\n",
        "        dtype=x.dtype\n",
        "    )\n",
        "\n",
        "    # Apply mask and scale\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = tf.ones((2, 10))\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].numpy()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].numpy()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = tf.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {tf.reduce_mean(samples).numpy():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with tf.Variable Only\n",
        "\n",
        "Before using Keras's layer system, let's build fully functional layers using only basic TensorFlow operations. This shows exactly what happens under the hood."
      ],
      "metadata": {
        "id": "4XqEZxTtWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only tf.Variable.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "            'softmax': lambda x: tf.nn.softmax(x, axis=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = tf.Variable(\n",
        "            tf.random.normal((in_features, out_features), stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = tf.Variable(\n",
        "                tf.zeros(out_features),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = tf.random.normal((2, 4))\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesc0NBxWcC_",
        "outputId": "53b38a47-5b6a-4a2f-ed55-08baeeda442f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  (2, 4)\n",
            "Output shape: (2, 3)\n",
            "Weight shape: (4, 3)\n",
            "Bias shape:   (3,)\n",
            "\n",
            "Output:\n",
            "[[0.         0.         0.46504724]\n",
            " [0.         0.         0.8249154 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only tf.Variable and tf.nn.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding='SAME', activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (height, width, in_channels, out_channels)\n",
        "        self.kernel = tf.Variable(\n",
        "            tf.random.normal((kernel_size[0], kernel_size[1], in_channels, out_channels),\n",
        "                           stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = tf.Variable(\n",
        "                tf.zeros(out_channels),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using tf.nn.conv2d\"\"\"\n",
        "        out = tf.nn.conv2d(x, self.kernel, strides=self.stride, padding=self.padding)\n",
        "        if self.use_bias:\n",
        "            out = out + self.bias\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, activation='relu')\n",
        "x = tf.random.normal((1, 28, 28, 3))  # 1 image, 28x28, 3 channels\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {np.prod(conv.kernel.shape) + conv.bias.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSWj-SXwWcC_",
        "outputId": "d9fdddc1-9908-46c1-9213-93453f2f8c01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3)\n",
            "Input shape:  (1, 28, 28, 3)\n",
            "Output shape: (1, 28, 28, 16)\n",
            "Kernel shape: (3, 3, 3, 16)\n",
            "Parameters:   448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[-1]\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With SAME padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[0] // 4, input_shape[1] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Flatten\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "\n",
        "        # Dense layers\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        variables = []\n",
        "        for layer in self.layers:\n",
        "            variables.extend(layer.trainable_variables)\n",
        "        return variables\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(np.prod(v.shape) for v in layer.trainable_variables)\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(28, 28, 1), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = tf.random.normal((4, 28, 28, 1))\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {tf.reduce_sum(y, axis=1).numpy()}  (should be ~1.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvCl_LZwWcC_",
        "outputId": "5e6ee34f-4c02-4e20-cd97-8c77017cc79d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  (4, 28, 28, 1)\n",
            "Output shape: (4, 10)\n",
            "Output sum per sample: [0.9999999 1.        1.        0.9999999]  (should be ~1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom Keras Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "Keras provides a clean API for custom layers with:\n",
        "- **`build()`**: Create weights when input shape is known\n",
        "- **`call()`**: Define the forward pass\n",
        "- **`get_config()`**: Enable serialization\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking\n",
        "- Serialization/deserialization\n",
        "- Integration with `model.fit()`\n",
        "- Proper shape inference"
      ],
      "metadata": {
        "id": "CPUeDPKIWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM KERAS LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM KERAS LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the Keras layer API.\n",
        "\n",
        "    Key methods:\n",
        "    - __init__: Store configuration (no weights yet!)\n",
        "    - build: Create weights when input shape is known\n",
        "    - call: Forward pass\n",
        "    - get_config: For serialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Create weights. Called automatically the first time the layer is used.\n",
        "\n",
        "        self.add_weight() creates a tf.Variable and registers it properly.\n",
        "        \"\"\"\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',  # Xavier initialization\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.units,),\n",
        "                initializer='zeros',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        # Mark as built\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = tf.matmul(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Enable serialization.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(32, activation='relu')\n",
        "x = tf.random.normal((4, 16))\n",
        "y = custom_dense(x)  # This triggers build()\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {custom_dense.kernel.shape}\")\n",
        "print(f\"Trainable variables: {len(custom_dense.trainable_variables)}\")\n",
        "print(f\"\\nConfig: {custom_dense.get_config()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnKJDd05WcC_",
        "outputId": "a1cd340e-ca98-48ea-d76f-6a2cabb2b2e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM KERAS LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(32, activation='relu')\n",
            "Input shape:  (4, 16)\n",
            "Output shape: (4, 32)\n",
            "Kernel shape: (16, 32)\n",
            "Trainable variables: 2\n",
            "\n",
            "Config: {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = self.add_weight(\n",
        "            name='W_q',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_k = self.add_weight(\n",
        "            name='W_k',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_v = self.add_weight(\n",
        "            name='W_v',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_o = self.add_weight(\n",
        "            name='W_o',\n",
        "            shape=(self.embed_dim, self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Linear projections\n",
        "        Q = inputs @ self.W_q  # (batch, seq, embed)\n",
        "        K = inputs @ self.W_k\n",
        "        V = inputs @ self.W_v\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = tf.reshape(Q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        K = tf.reshape(K, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        V = tf.reshape(V, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "\n",
        "        # Transpose to (batch, heads, seq, head_dim)\n",
        "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "        K = tf.transpose(K, [0, 2, 1, 3])\n",
        "        V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scale = tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / scale  # (batch, heads, seq, seq)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores += (1 - mask) * -1e9\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = tf.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])  # (batch, seq, heads, head_dim)\n",
        "        context = tf.reshape(context, (batch_size, seq_len, self.embed_dim))\n",
        "\n",
        "        # Output projection\n",
        "        output = context @ self.W_o\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = tf.random.normal((2, 10, 64))  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:            {x.shape}\")\n",
        "print(f\"Output shape:           {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:   {sum(np.prod(v.shape) for v in attention.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfKOP_WWWcDA",
        "outputId": "7c86d7eb-a610-42c0-d7be-e5fd3bce84b1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:            (2, 10, 64)\n",
            "Output shape:           (2, 10, 64)\n",
            "Attention weights shape: (2, 4, 10, 10)\n",
            "Trainable parameters:   16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNormalization(keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, power_iterations=1, epsilon=1e-12, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer.build(input_shape)\n",
        "\n",
        "        # Get the weight matrix\n",
        "        self.w = self.layer.kernel\n",
        "        w_shape = self.w.shape.as_list()\n",
        "\n",
        "        # Flatten weight to 2D for SVD\n",
        "        self.w_flat_shape = (np.prod(w_shape[:-1]), w_shape[-1])\n",
        "\n",
        "        # Initialize u vector (for power iteration)\n",
        "        self.u = self.add_weight(\n",
        "            name='u',\n",
        "            shape=(1, w_shape[-1]),\n",
        "            initializer='random_normal',\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        # Power iteration to estimate largest singular value\n",
        "        w_flat = tf.reshape(self.w, self.w_flat_shape)\n",
        "\n",
        "        u = self.u\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = tf.matmul(u, tf.transpose(w_flat))\n",
        "            v = v / (tf.norm(v, ord='euclidean') + self.epsilon)\n",
        "\n",
        "            # u = W v / ||W v||\n",
        "            u = tf.matmul(v, w_flat)\n",
        "            u = u / (tf.norm(u, ord='euclidean') + self.epsilon)\n",
        "\n",
        "        if training:\n",
        "            self.u.assign(u)\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        # Corrected calculation for row vectors u (1, out_features) and v (1, in_features)\n",
        "        # The formula should be v W u^T\n",
        "        sigma = tf.matmul(tf.matmul(v, w_flat), tf.transpose(u))\n",
        "\n",
        "        # Normalize weight\n",
        "        w_normalized = self.w / sigma[0, 0]\n",
        "\n",
        "        # Manually perform the forward pass of the wrapped layer using w_normalized.\n",
        "        # This assumes the wrapped layer is a Dense layer based on the test case.\n",
        "        output = tf.matmul(inputs, w_normalized)\n",
        "\n",
        "        # If the wrapped layer has a bias and uses it, add it.\n",
        "        if hasattr(self.layer, 'bias') and self.layer.use_bias:\n",
        "            output = output + self.layer.bias\n",
        "\n",
        "        # If the wrapped layer has an activation function, apply it.\n",
        "        if hasattr(self.layer, 'activation') and self.layer.activation is not None:\n",
        "            output = self.layer.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Test\n",
        "base_layer = layers.Dense(64)\n",
        "spectral_dense = SpectralNormalization(base_layer)\n",
        "x = tf.random.normal((4, 32))\n",
        "y = spectral_dense(x)\n",
        "\n",
        "print(f\"\\nSpectralNormalization(Dense(64))\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFYq5xJbWcDA",
        "outputId": "eebbefc1-1dcb-4d35-961b-c5b55c555172"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNormalization(Dense(64))\n",
            "Input shape:  (4, 32)\n",
            "Output shape: (4, 64)\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ],
      "metadata": {
        "id": "biOHWrOFWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size=3, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "\n",
        "        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip_conv = None\n",
        "        self.skip_bn = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Need projection if dimensions change\n",
        "        if input_shape[-1] != self.filters or self.stride != 1:\n",
        "            self.skip_conv = layers.Conv2D(self.filters, 1, strides=self.stride,\n",
        "                                           padding='same', use_bias=False)\n",
        "            self.skip_bn = layers.BatchNormalization()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Main path\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_conv is not None:\n",
        "            skip = self.skip_conv(inputs)\n",
        "            skip = self.skip_bn(skip, training=training)\n",
        "        else:\n",
        "            skip = inputs\n",
        "\n",
        "        # Add and activate\n",
        "        return tf.nn.relu(x + skip)\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, stride=1)\n",
        "x = tf.random.normal((2, 32, 32, 64))\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnNTvJTWcDA",
        "outputId": "76e56fd6-614d-43b2-8843-899fdfdcff77"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64)\n",
            "Input shape:  (2, 32, 32, 64)\n",
            "Output shape: (2, 32, 32, 64)\n",
            "\n",
            "ResidualBlock(128, stride=2)\n",
            "Output shape: (2, 16, 16, 128)  (spatial dims halved, channels doubled)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (H,W,C) -> (1,1,C)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        reduced_channels = max(channels // self.reduction_ratio, 1)\n",
        "\n",
        "        self.fc1 = layers.Dense(reduced_channels, activation='relu')\n",
        "        self.fc2 = layers.Dense(channels, activation='sigmoid')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)  # (B, 1, 1, C)\n",
        "\n",
        "        # Excitation\n",
        "        x = tf.reshape(squeezed, (tf.shape(inputs)[0], -1))  # (B, C)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.reshape(x, (tf.shape(inputs)[0], 1, 1, -1))  # (B, 1, 1, C)\n",
        "\n",
        "        # Scale\n",
        "        return inputs * x\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(reduction_ratio=16)\n",
        "x = tf.random.normal((2, 28, 28, 64))\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcm-_VhvWcDA",
        "outputId": "a2fa8745-0e86-43f3-c13c-c0da47527ed9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(reduction_ratio=16)\n",
            "Input shape:  (2, 28, 28, 64)\n",
            "Output shape: (2, 28, 28, 64)\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture:\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(embed_dim),\n",
        "            layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output = self.attention(x, x, attention_mask=mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        x = inputs + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn_input = self.layernorm2(x)\n",
        "        ffn_output = self.ffn(ffn_input, training=training)\n",
        "\n",
        "        return x + ffn_output  # Residual connection\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "x = tf.random.normal((2, 20, 64))  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(np.prod(v.shape) for v in transformer_block.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I-38YHnWcDA",
        "outputId": "5845dffe-1854-43b0-d259-89043e07cd49"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  (2, 20, 64)\n",
            "Output shape: (2, 20, 64)\n",
            "Parameters:   49,984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control with GradientTape\n",
        "\n",
        "While `model.fit()` is convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ],
      "metadata": {
        "id": "z13eQIQpWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(model, train_data, val_data, epochs, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Complete custom training loop with GradientTape.\n",
        "    \"\"\"\n",
        "    optimizer = keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = keras.metrics.Mean()\n",
        "    train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "    val_loss = keras.metrics.Mean()\n",
        "    val_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Reset metrics\n",
        "        train_loss.reset_state()\n",
        "        train_acc.reset_state()\n",
        "\n",
        "        # Training loop\n",
        "        for x_batch, y_batch in train_data:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass\n",
        "                predictions = model(x_batch, training=True)\n",
        "                loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss.update_state(loss)\n",
        "            train_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss.reset_state()\n",
        "        val_acc.reset_state()\n",
        "\n",
        "        for x_batch, y_batch in val_data:\n",
        "            predictions = model(x_batch, training=False)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "            val_loss.update_state(loss)\n",
        "            val_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss.result().numpy())\n",
        "        history['train_acc'].append(train_acc.result().numpy())\n",
        "        history['val_loss'].append(val_loss.result().numpy())\n",
        "        history['val_acc'].append(val_acc.result().numpy())\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss.result():.4f} | \"\n",
        "              f\"Train Acc: {train_acc.result():.4f} | \"\n",
        "              f\"Val Loss: {val_loss.result():.4f} | \"\n",
        "              f\"Val Acc: {val_acc.result():.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. GradientTape for computing gradients\")\n",
        "print(\"  2. optimizer.apply_gradients() for weight updates\")\n",
        "print(\"  3. Metrics for tracking performance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7sZ5woWcDA",
        "outputId": "8fd84882-1526-47a1-de36-39ee9dd2870a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. GradientTape for computing gradients\n",
            "  2. optimizer.apply_gradients() for weight updates\n",
            "  3. Metrics for tracking performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "@tf.function\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = loss_fn(y, predictions)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, global_norm\n",
        "\n",
        "print(\"Gradient Clipping Options:\")\n",
        "print(\"  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\")\n",
        "print(\"  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\")\n",
        "print(\"  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt4KslvnWcDA",
        "outputId": "a519afae-0881-4196-88ed-174a39d87ff3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options:\n",
            "  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\n",
            "  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\n",
            "  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM MODEL WITH train_step()\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        CUSTOM MODEL WITH train_step()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomTrainableModel(keras.Model):\n",
        "    \"\"\"\n",
        "    Model with custom training logic built-in.\n",
        "\n",
        "    Override train_step() to customize what happens in model.fit().\n",
        "    This is the best of both worlds:\n",
        "    - Custom training logic\n",
        "    - Still use model.fit() with callbacks, validation, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units_list, num_classes, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.dense_layers = []\n",
        "        for units in units_list:\n",
        "            self.dense_layers.append(layers.Dense(units, activation='relu'))\n",
        "            self.dense_layers.append(layers.Dropout(0.2))\n",
        "\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "        # Custom metrics\n",
        "        self.loss_tracker = keras.metrics.Mean(name='loss')\n",
        "        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom training step.\n",
        "        Called by model.fit() for each batch.\n",
        "        \"\"\"\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Compute and apply gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Custom evaluation step.\"\"\"\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "\n",
        "        self.loss_tracker.update_state(tf.reduce_mean(loss))\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "# Test\n",
        "custom_model = CustomTrainableModel([64, 32], num_classes=10)\n",
        "custom_model.compile(optimizer='adam')\n",
        "\n",
        "print(\"\\nCustom model with train_step() created!\")\n",
        "print(\"Now model.fit() uses our custom training logic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ocCaxVeWcDA",
        "outputId": "fcfded89-3a0f-432a-c770-4419785bfc79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        CUSTOM MODEL WITH train_step()\n",
            "============================================================\n",
            "\n",
            "Custom model with train_step() created!\n",
            "Now model.fit() uses our custom training logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in real examples."
      ],
      "metadata": {
        "id": "S6cCw5HyWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#              DEMO 1: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 8, 8, 1)\n",
        "X = X.reshape(-1, 8, 8, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")\n",
        "\n",
        "# Build custom ResNet model\n",
        "class MiniResNet(keras.Model):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32)\n",
        "        self.res_block2 = ResidualBlock(64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.res_block1(x, training=training)\n",
        "        x = self.res_block2(x, training=training)\n",
        "        x = self.se_block(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.dense(x)\n",
        "\n",
        "# Create and compile\n",
        "tf.random.set_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10)\n",
        "\n",
        "resnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build model by calling it\n",
        "_ = resnet_model(X_train[:1])\n",
        "resnet_model.summary()\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining MiniResNet...\")\n",
        "history = resnet_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = resnet_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hJng8MQqWcDA",
        "outputId": "98cf78f0-96b6-4b47-dcca-fcdf5c3eb64b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      (8, 8, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mini_res_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_res_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m320\u001b[0m \n",
              "\n",
              " residual_block_2                 ?                              \u001b[38;5;34m18,688\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " residual_block_3                 ?                              \u001b[38;5;34m58,112\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               \u001b[38;5;34m1,096\u001b[0m \n",
              " (\u001b[38;5;33mSqueezeExcitationBlock\u001b[0m)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
              "\n",
              " dropout_6 (\u001b[38;5;33mDropout\u001b[0m)              ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m650\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
              "\n",
              " residual_block_2                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " residual_block_3                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">58,112</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,096</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SqueezeExcitationBlock</span>)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
              "\n",
              " dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,866\u001b[0m (308.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,866</span> (308.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,354\u001b[0m (306.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,354</span> (306.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 45ms/step - accuracy: 0.3633 - loss: 2.0106 - val_accuracy: 0.4972 - val_loss: 2.1791\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8949 - loss: 0.7307 - val_accuracy: 0.2306 - val_loss: 1.9517\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9536 - loss: 0.2720 - val_accuracy: 0.1583 - val_loss: 2.0069\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9884 - loss: 0.1237 - val_accuracy: 0.2111 - val_loss: 2.1717\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9884 - loss: 0.0742 - val_accuracy: 0.4639 - val_loss: 1.3992\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9923 - loss: 0.0483 - val_accuracy: 0.5639 - val_loss: 1.1632\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9999 - loss: 0.0311 - val_accuracy: 0.8333 - val_loss: 0.4853\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9982 - loss: 0.0262 - val_accuracy: 0.9528 - val_loss: 0.1909\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9983 - loss: 0.0260 - val_accuracy: 0.9861 - val_loss: 0.0664\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 0.9833 - val_loss: 0.0752\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9972 - loss: 0.0219 - val_accuracy: 0.9500 - val_loss: 0.1180\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0210 - val_accuracy: 0.9833 - val_loss: 0.0564\n",
            "Epoch 13/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 0.0131 - val_accuracy: 0.9944 - val_loss: 0.0377\n",
            "Epoch 14/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 0.9889 - val_loss: 0.0471\n",
            "Epoch 15/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9986 - loss: 0.0105 - val_accuracy: 0.9917 - val_loss: 0.0440\n",
            "Epoch 16/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0093 - val_accuracy: 0.9944 - val_loss: 0.0333\n",
            "Epoch 17/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9944 - val_loss: 0.0294\n",
            "Epoch 18/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9944 - val_loss: 0.0242\n",
            "Epoch 19/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.9917 - val_loss: 0.0294\n",
            "Epoch 20/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9917 - val_loss: 0.0305\n",
            "Epoch 21/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.9917 - val_loss: 0.0332\n",
            "Epoch 22/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9988 - loss: 0.0059 - val_accuracy: 0.9861 - val_loss: 0.0335\n",
            "Epoch 23/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.9917 - val_loss: 0.0283\n",
            "Epoch 24/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9889 - val_loss: 0.0314\n",
            "Epoch 25/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9889 - val_loss: 0.0262\n",
            "Epoch 26/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9944 - val_loss: 0.0271\n",
            "Epoch 27/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9889 - val_loss: 0.0267\n",
            "Epoch 28/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.9889 - val_loss: 0.0258\n",
            "Epoch 29/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9944 - val_loss: 0.0286\n",
            "Epoch 30/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9889 - val_loss: 0.0232\n",
            "\n",
            "Test Accuracy: 98.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history.history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], 'b-', label='Train Acc')\n",
        "ax2.plot(history.history['val_accuracy'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "predictions = resnet_model.predict(X_test[:10], verbose=0)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i].reshape(8, 8)\n",
        "    true_label = y_test[i]\n",
        "    pred_label = pred_classes[i]\n",
        "    confidence = predictions[i][pred_label] * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6QkXz-l8WcDA",
        "outputId": "a11d7157-560b-42ab-8dbf-3a12036e3f75"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "GRADIENTTAPE PATTERNS\n",
        "---------------------\n",
        "# Basic gradient\n",
        "with tf.GradientTape() as tape:\n",
        "    y = model(x)\n",
        "grads = tape.gradient(y, model.trainable_variables)\n",
        "\n",
        "# Higher-order derivatives (nested tapes)\n",
        "with tf.GradientTape() as t2:\n",
        "    with tf.GradientTape() as t1:\n",
        "        y = f(x)\n",
        "    dy = t1.gradient(y, x)\n",
        "d2y = t2.gradient(dy, x)\n",
        "\n",
        "# Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "# Custom gradient\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    def grad(dy):\n",
        "        return dy * custom_backward\n",
        "    return forward_result, grad\n",
        "\n",
        "CUSTOM KERAS LAYERS\n",
        "-------------------\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return inputs @ self.kernel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['units'] = self.units\n",
        "        return config\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Override train_step for model.fit()\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Z_iXlFWcDA",
        "outputId": "b3d12013-ba25-4d70-9d09-a81e76e5d968"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "GRADIENTTAPE PATTERNS\n",
            "---------------------\n",
            "# Basic gradient\n",
            "with tf.GradientTape() as tape:\n",
            "    y = model(x)\n",
            "grads = tape.gradient(y, model.trainable_variables)\n",
            "\n",
            "# Higher-order derivatives (nested tapes)\n",
            "with tf.GradientTape() as t2:\n",
            "    with tf.GradientTape() as t1:\n",
            "        y = f(x)\n",
            "    dy = t1.gradient(y, x)\n",
            "d2y = t2.gradient(dy, x)\n",
            "\n",
            "# Jacobian\n",
            "jacobian = tape.jacobian(y, x)\n",
            "\n",
            "# Custom gradient\n",
            "@tf.custom_gradient\n",
            "def custom_op(x):\n",
            "    def grad(dy):\n",
            "        return dy * custom_backward\n",
            "    return forward_result, grad\n",
            "\n",
            "CUSTOM KERAS LAYERS\n",
            "-------------------\n",
            "class CustomLayer(keras.layers.Layer):\n",
            "    def __init__(self, units, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.units = units\n",
            "\n",
            "    def build(self, input_shape):\n",
            "        self.kernel = self.add_weight(\n",
            "            shape=(input_shape[-1], self.units),\n",
            "            initializer='glorot_uniform',\n",
            "            trainable=True\n",
            "        )\n",
            "        super().build(input_shape)\n",
            "\n",
            "    def call(self, inputs, training=False):\n",
            "        return inputs @ self.kernel\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super().get_config()\n",
            "        config['units'] = self.units\n",
            "        return config\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Override train_step for model.fit()\n",
            "class CustomModel(keras.Model):\n",
            "    def train_step(self, data):\n",
            "        x, y = data\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = self(x, training=True)\n",
            "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
            "        grads = tape.gradient(loss, self.trainable_variables)\n",
            "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
            "        return {'loss': loss}\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced TensorFlow & Keras Journey\n",
        "\n",
        "Congratulations! You've mastered advanced TensorFlow and Keras techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only tf.Variable |\n",
        "| IV | Custom Keras Layers | Proper subclassing with build() and call() |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with GradientTape |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| `model.fit()` | Standard training, quick prototyping |\n",
        "| Custom `train_step()` | Custom logic but want callbacks/validation |\n",
        "| Full GradientTape loop | GANs, RL, complex multi-model training |\n",
        "| Custom layers | Reusable components, research |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch** - Research-friendly framework\n",
        "3. **TensorFlow/Keras Part 1** - Fundamentals and high-level API\n",
        "4. **TensorFlow/Keras Part 2** - Advanced custom components (This notebook!)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Neural Architecture Search** - Automated model design\n",
        "- **Quantization & Pruning** - Model optimization for deployment\n",
        "- **TensorFlow Extended (TFX)** - Production ML pipelines\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ],
      "metadata": {
        "id": "wQs6OOHGWcDA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}